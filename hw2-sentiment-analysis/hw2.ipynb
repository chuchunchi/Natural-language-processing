{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_map(label):\n",
    "    if label == \"neutral\":\n",
    "        return 0\n",
    "    elif label == \"anger\":\n",
    "        return 1\n",
    "    elif label == \"joy\":\n",
    "        return 2\n",
    "    elif label == \"surprise\":\n",
    "        return 3\n",
    "    elif label == \"sadness\":\n",
    "        return 4\n",
    "    elif label == \"disgust\":\n",
    "        return 5\n",
    "    elif label == \"fear\":\n",
    "        return 6\n",
    "\n",
    "s2idx = {\"Chandler\":0, \"Joey\":1, \"Rachel\":2, \"Monica\":3, \"Phoebe\":4, \"Ross\":5}   \n",
    "    \n",
    "def encode(text, word2index, label, N, speaker):\n",
    "    tokenized = word_tokenize(text)\n",
    "    encoded = [0]*N\n",
    "    enc1 = [word2index.get(word) for word in tokenized]\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    '''if(speaker in s2idx):\n",
    "        idx = s2idx[speaker]\n",
    "    else:\n",
    "        idx = 6\n",
    "    encoded.insert(0,idx)'''\n",
    "    return (encoded,label)\n",
    "\n",
    "def encode_test(text, word2index, N, speaker):\n",
    "    tokenized = word_tokenize(text)\n",
    "    for i,word in enumerate(tokenized):\n",
    "        if word2index.get(word)==None:\n",
    "            tokenized[i]='unk'\n",
    "\n",
    "    encoded = [0]*N\n",
    "    enc1 = [word2index.get(word) for word in tokenized]\n",
    "\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    '''if(speaker in s2idx):\n",
    "        idx = s2idx[speaker]\n",
    "    else:\n",
    "        idx = 6\n",
    "    encoded.insert(0,idx)'''\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text = batch[0].to(device)\n",
    "        target = batch[1]\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target = target.to(device)\n",
    "        preds = model(text)\n",
    "        loss = criterion(preds, target)\n",
    "        _, pred = torch.max(preds, 1)\n",
    "        acc = accuracy_score(pred.tolist(), target.tolist())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    for batch in iterator:\n",
    "        text = batch[0].to(device)\n",
    "        target = batch[1]\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target = target.to(device)\n",
    "        preds = model(text)\n",
    "        loss = criterion(preds, target)\n",
    "        _, pred = torch.max(preds, 1)\n",
    "        #print(pred)\n",
    "        acc = accuracy_score(pred.tolist(), target.tolist())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'batch_size = 32\\n\\ntrain_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\\ndev_ds = TensorDataset(torch.from_numpy(dev_x), torch.from_numpy(dev_y))\\n\\ntrain_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)\\ndev_dl = DataLoader(dev_ds, shuffle=True, batch_size=batch_size, drop_last=True)'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_HW2dataset.csv')\n",
    "dev_df = pd.read_csv('dev_HW2dataset.csv')\n",
    "\n",
    "train_df=train_df[['Emotion','Utterance','Speaker']]\n",
    "dev_df=dev_df[['Emotion','Utterance','Speaker']]\n",
    "\n",
    "train_set = list(train_df.to_records(index=False))\n",
    "dev_set = list(dev_df.to_records(index=False))\n",
    "ps = PorterStemmer()\n",
    "lm = WordNetLemmatizer()\n",
    "counts = Counter()\n",
    "\n",
    "new_train_set = []\n",
    "for label,text,speaker in train_set:\n",
    "    #newText = [ps.stem(w) for w in text]\n",
    "    #newText = [lm.lemmatize(w) for w in text]\n",
    "    newText = [w.lower() for w in text]\n",
    "    new_train_set.append((label,text,speaker))\n",
    "new_dev_set = []\n",
    "for label,text,speaker in dev_set:\n",
    "    #newText = [ps.stem(w) for w in text]\n",
    "    #newText = [lm.lemmatize(w) for w in text]\n",
    "    newText = [w.lower() for w in text]\n",
    "    new_dev_set.append((label,text,speaker))\n",
    "\n",
    "for ds in [train_set, dev_set]:\n",
    "    for label,text,speaker in ds:\n",
    "        counts.update(word_tokenize(text))\n",
    "\n",
    "word2index = {'unk':0}\n",
    "for i,word in enumerate(counts.keys()):\n",
    "    word2index[word] = i+1\n",
    "index2word = {v:k for k,v in word2index.items()}\n",
    "\n",
    "train_encoded = [(encode(Utterance,word2index,label_map(label),15,s)) for label, Utterance, s in train_set]\n",
    "dev_encoded   = [(encode(Utterance,word2index,label_map(label),15,s)) for label, Utterance,s in dev_set]\n",
    "\n",
    "train_x = np.array([tweet for tweet, label in train_encoded])\n",
    "train_y = np.array([label for tweet, label in train_encoded])\n",
    "dev_x = np.array([tweet for tweet, label in dev_encoded])\n",
    "dev_y = np.array([label for tweet, label in dev_encoded])\n",
    "\n",
    "'''batch_size = 32\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "dev_ds = TensorDataset(torch.from_numpy(dev_x), torch.from_numpy(dev_y))\n",
    "\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "dev_dl = DataLoader(dev_ds, shuffle=True, batch_size=batch_size, drop_last=True)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_x = np.append(train_x, dev_x, axis=0)\n",
    "total_y = np.append(train_y, dev_y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(word2index)\n",
    "dimension_model = 300\n",
    "num_layers = 5\n",
    "hidden_size = 200\n",
    "linear_hidden_size = 30\n",
    "classes = 7\n",
    "dropout = 0.2        \n",
    "lr = 1e-3\n",
    "attention_width = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embeddings = [[]]*src_vocab_size\\ncount = 0\\nwith open(\\'glove.42B.300d.txt\\',\\'rt\\',encoding=\"utf-8\") as fi:\\n    for line in fi:\\n        i_word = line.split(\\' \\')[0]\\n        i_embeddings = [float(val) for val in line.split(\\' \\')[1:]]\\n        if(i_word in word2index):\\n            count+=1\\n            embeddings[word2index[i_word]] = i_embeddings\\nfor i,emb in enumerate(embeddings):\\n    if emb == []:\\n        embeddings[i] = [0]*dimension_model\\nembeddings[word2index[\\'unk\\']] = [0]*dimension_model\\nembs_npa = np.array(embeddings)\\n\\nwith open(\\'embs_npa.npy\\',\\'wb\\') as f:\\n    np.save(f,embs_npa)'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''embeddings = [[]]*src_vocab_size\n",
    "count = 0\n",
    "with open('glove.42B.300d.txt','rt',encoding=\"utf-8\") as fi:\n",
    "    for line in fi:\n",
    "        i_word = line.split(' ')[0]\n",
    "        i_embeddings = [float(val) for val in line.split(' ')[1:]]\n",
    "        if(i_word in word2index):\n",
    "            count+=1\n",
    "            embeddings[word2index[i_word]] = i_embeddings\n",
    "for i,emb in enumerate(embeddings):\n",
    "    if emb == []:\n",
    "        embeddings[i] = [0]*dimension_model\n",
    "embeddings[word2index['unk']] = [0]*dimension_model\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "with open('embs_npa.npy','wb') as f:\n",
    "    np.save(f,embs_npa)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6933\n"
     ]
    }
   ],
   "source": [
    "print(src_vocab_size)\n",
    "#print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_npa = np.load('embs_npa.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def batch_matmul(seq, weight, nonlinearity=''):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s = torch.tanh(_s)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "class AttentionLayer(torch.nn.Module):\n",
    "    \"\"\"Implements an Attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, layer_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.weight_W = torch.nn.Parameter(torch.Tensor(layer_size,layer_size))\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(layer_size))\n",
    "        self.weight_proj = torch.nn.Parameter(torch.Tensor(layer_size, 1))\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.weight_W.data.uniform_(-0.1, 0.1)\n",
    "        self.weight_proj.data.uniform_(-0.1,0.1)\n",
    "\n",
    "    def forward(self, inputs, attention_width=3):\n",
    "        results = None\n",
    "        for i in range(inputs.size(0)):\n",
    "            if(i<attention_width):\n",
    "                output = inputs[i]\n",
    "                output = output.unsqueeze(0)\n",
    "            else:\n",
    "                lb = i - attention_width\n",
    "                if(lb<0):\n",
    "                    lb = 0\n",
    "                selector = torch.from_numpy(np.array(np.arange(lb, i)))\n",
    "                selector = Variable(selector)\n",
    "                selector = selector.cuda()\n",
    "                vec = torch.index_select(inputs, 0, selector)\n",
    "                #print(vec.shape, self.weight_W.shape)\n",
    "                u = batch_matmul(vec, self.weight_W, nonlinearity='tanh')\n",
    "                a = batch_matmul(u, self.weight_proj)\n",
    "                a = self.softmax(a)\n",
    "                output = None\n",
    "                for i in range(vec.size(0)):\n",
    "                    \n",
    "                    h_i = vec[i]\n",
    "                    #print(h_i.shape)\n",
    "                    a_i = a[i].unsqueeze(1).expand_as(h_i)\n",
    "                    h_i = a_i * h_i\n",
    "                    h_i = h_i.unsqueeze(0)\n",
    "                    if(output is None):\n",
    "                        output = h_i\n",
    "                    else:\n",
    "                        output = torch.cat((output,h_i),0)\n",
    "                #print(output.size())\n",
    "                #output = torch.sum(output,0)\n",
    "                #print(output.size())\n",
    "            if(results is None):\n",
    "                results = output\n",
    "            else:\n",
    "                #print(results.shape, output.shape)\n",
    "                results = torch.cat((results,output),0)\n",
    "            # print(results.size())\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        #self.embed = torch.nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float())\n",
    "        self.embed = torch.nn.Embedding(src_vocab_size, dimension_model)                  \n",
    "        self.lstm = torch.nn.LSTM(input_size=dimension_model, hidden_size=hidden_size,num_layers=num_layers,dropout=dropout)\n",
    "        self.linear = torch.nn.Linear(hidden_size, classes)\n",
    "        #self.linear1 = torch.nn.Linear(linear_hidden_size, classes)\n",
    "        self.AttentionLayer = AttentionLayer(hidden_size)\n",
    "    def forward(self,data):\n",
    "        x = self.embed(data)                          \n",
    "        x,(h_n, c_n) = self.lstm(x.transpose(0, 1))  \n",
    "        #attention\n",
    "        #print(x.shape)\n",
    "        #print(h_n.shape)\n",
    "        #x = self.AttentionLayer(x,attention_width=attention_width)                                          \n",
    "                                                    \n",
    "        x = self.linear(x[-1])                          \n",
    "        #x = self.linear1(x)                            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "def cv_idx():\n",
    "    idxarr = np.arange(len(total_y))\n",
    "    np.random.shuffle(idxarr)\n",
    "    Q = int( len(total_y) / k )\n",
    "    #print(Q)\n",
    "    rem = len(total_y) % k\n",
    "    \n",
    "    ret = [[] for _ in range(k)]\n",
    "    foldsize = np.zeros(k)\n",
    "    for i in range(k):\n",
    "        if i < rem:\n",
    "            foldsize[i] = Q + 1\n",
    "        else:\n",
    "            foldsize[i] = Q\n",
    "    for i in range(k):\n",
    "        train = []\n",
    "        val = []\n",
    "        start = 0\n",
    "        for j in range(k): \n",
    "            if j == i:\n",
    "                for s in range(int(foldsize[j])):\n",
    "                    val.append(idxarr[start])\n",
    "                    start += 1\n",
    "            else:\n",
    "                for s in range(int(foldsize[j])):\n",
    "                    train.append(idxarr[start])\n",
    "                    start += 1\n",
    "        ret[i].append(np.array(train))\n",
    "        ret[i].append(np.array(val))\n",
    "    return ret\n",
    "\n",
    "kfold_data = cv_idx()\n",
    "\n",
    "def cross_val(i):\n",
    "    batch_size = 32\n",
    "    train_x = total_x[kfold_data[i][0]]\n",
    "    train_y = total_y[kfold_data[i][0]]\n",
    "    dev_x = total_x[kfold_data[i][1]]\n",
    "    dev_y = total_y[kfold_data[i][1]]\n",
    "    train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    dev_ds = TensorDataset(torch.from_numpy(dev_x), torch.from_numpy(dev_y))\n",
    "\n",
    "    train_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    dev_dl = DataLoader(dev_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    return train_dl, dev_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 1.433, Train Acc: 51.60%, Val. Loss: 1.423, Val. Acc: 51.90%\n",
      "Epoch: 02, Train Loss: 1.334, Train Acc: 53.25%, Val. Loss: 1.357, Val. Acc: 52.53%\n",
      "Epoch: 03, Train Loss: 1.208, Train Acc: 57.77%, Val. Loss: 1.301, Val. Acc: 54.88%\n",
      "Epoch: 04, Train Loss: 1.056, Train Acc: 64.67%, Val. Loss: 1.266, Val. Acc: 57.83%\n",
      "Epoch: 05, Train Loss: 0.909, Train Acc: 70.69%, Val. Loss: 1.323, Val. Acc: 60.04%\n",
      "Epoch: 06, Train Loss: 0.777, Train Acc: 75.77%, Val. Loss: 1.297, Val. Acc: 62.22%\n",
      "Epoch: 07, Train Loss: 0.663, Train Acc: 79.25%, Val. Loss: 1.327, Val. Acc: 61.27%\n",
      "Epoch: 08, Train Loss: 0.580, Train Acc: 81.82%, Val. Loss: 1.399, Val. Acc: 62.71%\n",
      "Epoch: 09, Train Loss: 0.499, Train Acc: 84.32%, Val. Loss: 1.372, Val. Acc: 63.41%\n",
      "Epoch: 10, Train Loss: 0.435, Train Acc: 86.33%, Val. Loss: 1.403, Val. Acc: 64.26%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "total_v_acc = 0\n",
    "best_acc = 0\n",
    "\n",
    "for i in range(k):\n",
    "    model = LSTM().to(device)                           \n",
    "    criterion = torch.nn.CrossEntropyLoss()                  \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr) \n",
    "\n",
    "    train_dl, dev_dl = cross_val(i)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(10):\n",
    "        train_loss, train_acc = train(model, train_dl,\n",
    "                                        optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, dev_dl,\n",
    "                                            criterion)\n",
    "        \n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc * 100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc * 100:.2f}%')    \n",
    "        if best_acc <= valid_acc:\n",
    "            best_acc = valid_acc\n",
    "            PATH=f\"epoch{epoch+1}_{i}_val.accuracy{valid_acc:.3f}%.pt\"\n",
    "            torch.save({\n",
    "                    'epoch': epoch+1,\n",
    "                    'i': i,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': valid_loss,\n",
    "                    }, PATH)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_HW2dataset.csv')\n",
    "test_df=test_df[['Utterance','Speaker']]\n",
    "#test_set = test_df.values.tolist()\n",
    "test_set = list(test_df.to_records(index=False))\n",
    "new_test_set = []\n",
    "for text,speaker in test_set:\n",
    "    #newText = [ps.stem(w) for w in text]\n",
    "    #newText = [lm.lemmatize(w) for w in text]\n",
    "    newText = [w.lower() for w in text]\n",
    "    new_test_set.append((text,speaker))\n",
    "#print(test_set)\n",
    "test_encoded=[]\n",
    "test_encoded+=[encode_test(Utterance, word2index, 10, s) for Utterance,s in test_set]\n",
    "test_x = np.array(test_encoded)\n",
    "test_ds = TensorDataset(torch.from_numpy(test_x))\n",
    "test_dl = DataLoader(test_ds, shuffle=False, batch_size=2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "model = LSTM().to(device)                               \n",
    "criterion = torch.nn.CrossEntropyLoss()                  \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr) \n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "model.eval()\n",
    "predict=[]\n",
    "a = 0\n",
    "for deta in test_dl:\n",
    "    a+=1\n",
    "    text = deta[0].to(device)\n",
    "    #print(text)\n",
    "    preds = model(text)\n",
    "    _, pred = torch.max(preds, 1)\n",
    "    for i in pred:\n",
    "        #print(i.item())\n",
    "        predict.append(i.item())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3400\n"
     ]
    }
   ],
   "source": [
    "print(len(predict))\n",
    "ans = [[i, pre] for [i,pre] in enumerate(predict)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "h = ['index', 'emotion']\n",
    "with open('predict.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(h)\n",
    "    writer.writerows(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab78bb3c465f8a9a3b2b3033a0b53f8ddb3d4f2700280c40496e40ae689e591d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
