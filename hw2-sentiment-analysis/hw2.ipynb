{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_map(label):\n",
    "    if label == \"neutral\":\n",
    "        return 0\n",
    "    elif label == \"anger\":\n",
    "        return 1\n",
    "    elif label == \"joy\":\n",
    "        return 2\n",
    "    elif label == \"surprise\":\n",
    "        return 3\n",
    "    elif label == \"sadness\":\n",
    "        return 4\n",
    "    elif label == \"disgust\":\n",
    "        return 5\n",
    "    elif label == \"fear\":\n",
    "        return 6\n",
    "    \n",
    "def encode(text, word2index, label, N):\n",
    "    tokenized = word_tokenize(text)\n",
    "    encoded = [0]*N\n",
    "    enc1 = [word2index.get(word) for word in tokenized]\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return (encoded,label)\n",
    "\n",
    "def encode_test(text, word2index, N):\n",
    "    tokenized = word_tokenize(text)\n",
    "    for i,word in enumerate(tokenized):\n",
    "        if word2index.get(word)==None:\n",
    "            tokenized[i]='unk'\n",
    "\n",
    "    encoded = [0]*N\n",
    "    enc1 = [word2index.get(word) for word in tokenized]\n",
    "\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text = batch[0].to(device)\n",
    "        target = batch[1]\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target = target.to(device)\n",
    "        preds = model(text)\n",
    "        loss = criterion(preds, target)\n",
    "        _, pred = torch.max(preds, 1)\n",
    "        acc = accuracy_score(pred.tolist(), target.tolist())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    for batch in iterator:\n",
    "        text = batch[0].to(device)\n",
    "        target = batch[1]\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target = target.to(device)\n",
    "        preds = model(text)\n",
    "        loss = criterion(preds, target)\n",
    "        _, pred = torch.max(preds, 1)\n",
    "        acc = accuracy_score(pred.tolist(), target.tolist())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_HW2dataset.csv')\n",
    "dev_df = pd.read_csv('dev_HW2dataset.csv')\n",
    "\n",
    "train_df=train_df[['Emotion','Utterance']]\n",
    "dev_df=dev_df[['Emotion','Utterance']]\n",
    "\n",
    "train_set = list(train_df.to_records(index=False))\n",
    "dev_set = list(dev_df.to_records(index=False))\n",
    "\n",
    "counts = Counter()\n",
    "for ds in [train_set, dev_set]:\n",
    "    for label,text in ds:\n",
    "        counts.update(word_tokenize(text))\n",
    "\n",
    "word2index = {'unk':0}\n",
    "for i,word in enumerate(counts.keys()):\n",
    "    word2index[word] = i+1\n",
    "index2word = {v:k for k,v in word2index.items()}\n",
    "\n",
    "train_encoded = [(encode(Utterance,word2index,label_map(label),12)) for label, Utterance in train_set]\n",
    "dev_encoded   = [(encode(Utterance,word2index,label_map(label),12)) for label, Utterance in dev_set]\n",
    "\n",
    "train_x = np.array([tweet for tweet, label in train_encoded])\n",
    "train_y = np.array([label for tweet, label in train_encoded])\n",
    "dev_x = np.array([tweet for tweet, label in dev_encoded])\n",
    "dev_y = np.array([label for tweet, label in dev_encoded])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "dev_ds = TensorDataset(torch.from_numpy(dev_x), torch.from_numpy(dev_y))\n",
    "\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "dev_dl = DataLoader(dev_ds, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(word2index)      \n",
    "dimension_model = 32                             \n",
    "num_layers = 5                       \n",
    "hidden_size = 30                         \n",
    "linear_hidden_size = 10               \n",
    "classes = 7                          \n",
    "dropout = 0.2                            \n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(src_vocab_size, dimension_model)                        \n",
    "        self.lstm = torch.nn.LSTM(input_size=dimension_model, hidden_size=hidden_size,num_layers=num_layers,dropout=dropout)\n",
    "        self.linear = torch.nn.Linear(hidden_size, linear_hidden_size)\n",
    "        self.linear1 = torch.nn.Linear(linear_hidden_size, classes)\n",
    "    def forward(self,data):\n",
    "        x = self.embed(data)                          \n",
    "        x,(h_n, c_n) = self.lstm(x.transpose(0, 1))  \n",
    "                                                         \n",
    "                                                    \n",
    "        x = self.linear(x[-1])                          \n",
    "        x = self.linear1(x)                            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "model = LSTM().to(device)                           \n",
    "criterion = torch.nn.CrossEntropyLoss()                  \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr) \n",
    "\n",
    "best_acc = 0\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train(model, train_dl,\n",
    "                                  optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, dev_dl,\n",
    "                                     criterion)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc * 100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc * 100:.2f}%')    \n",
    "    if best_acc <= valid_acc:\n",
    "        best_acc = valid_acc\n",
    "        PATH=f\"epoch{epoch+1}_val.accuracy{valid_loss:.3f}%.pt\"\n",
    "        torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': valid_loss,\n",
    "                }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_HW2dataset.csv')\n",
    "test_df=test_df[['Utterance']]\n",
    "test_set = test_df.values.tolist()\n",
    "test_encoded=[]\n",
    "for sentence in test_set:\n",
    "    test_encoded+=[encode_test(Utterance, word2index, 10) for Utterance in sentence]\n",
    "test_x = np.array(test_encoded)\n",
    "test_ds = TensorDataset(torch.from_numpy(test_x))\n",
    "test_dl = DataLoader(test_ds, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "model = LSTM().to(device)                               \n",
    "criterion = torch.nn.CrossEntropyLoss()                  \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr) \n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "model.eval()\n",
    "predict=[]\n",
    "for deta in test_dl:\n",
    "    text = deta[0].to(device)\n",
    "    preds = model(text)\n",
    "    _, pred = torch.max(preds, 1)\n",
    "    predict.append(pred.item())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
